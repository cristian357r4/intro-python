{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKWXreTj95Z6"
      },
      "source": [
        "# Complejidad de los datos\n",
        "\n",
        "### Introducción\n",
        "\n",
        "En esta sesión analizaremos el tema de complejidad de los datos y algunas técnicas para tratar los efectos de la misma. Este cuaderno se basa parcialmente en el material del curso de limpieza de datos de Kaggle disponible [aquí](https://www.kaggle.com/learn/data-cleaning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8qKe50s95Z-"
      },
      "source": [
        "### Gestión de valores omitidos\n",
        "Elimine los valores que faltan o rellénelos con un flujo de trabajo automatizado.\n",
        "\n",
        "La limpieza de datos es una parte clave de la ciencia de datos, pero puede ser muy frustrante. ¿Por qué hay campos de texto ilegibles? ¿Qué hacer con los valores que faltan? ¿Por qué las fechas no tienen el formato correcto? ¿Cómo puede solucionar rápidamente la introducción de datos incoherentes? En este tema, aprenderá por qué se ha encontrado con estos problemas y, lo que es más importante, cómo solucionarlos.\n",
        "\n",
        "En este cuaderno, aprenderá a abordar algunos de los problemas más comunes de limpieza de datos para que pueda analizar sus datos más rápidamente. Realizará cinco ejercicios prácticos con datos reales y desordenados y responderá a algunas de las preguntas más frecuentes sobre la limpieza de datos.\n",
        "\n",
        "En este cuaderno, veremos cómo tratar los valores faltantes u omitidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bySSHnJZ95Z-"
      },
      "source": [
        "#### Primer vistazo a los datos\n",
        "\n",
        "Lo primero que tenemos que hacer es cargar las bibliotecas y el conjunto de datos que vamos a utilizar.\n",
        "\n",
        "Para la demostración, utilizaremos un conjunto de datos de eventos ocurridos en partidos de fútbol americano. Debido al tamaño del conjunto de datos, lo descargaremos y posteriormente lo cargaremos a nuestro espacio temporal. [Ir a la página de descarga](https://www.kaggle.com/code/alexisbcook/handling-missing-values/data?select=NFL+Play+by+Play+2009-2017+%28v4%29.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOMBSNG095Z_",
        "outputId": "4d6bd9f6-3029-4cf4-9381-164958c6442c"
      },
      "outputs": [],
      "source": [
        "# módulos que usaremos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# cargamos los datos\n",
        "nfl_data = pd.read_csv(\"NFL Play by Play 2009-2017 (v4).csv\")\n",
        "\n",
        "# fijamos la semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEI_X99_95aA"
      },
      "source": [
        "Lo primero que hay que hacer cuando se recibe un nuevo conjunto de datos es echar un vistazo a algunos de ellos. Esto nos permite ver que todo se lee correctamente y nos da una idea de lo que está pasando con los datos. En este caso, vamos a ver si hay valores perdidos u omitidos, que son representados en Python con `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "ILNIu14E95aA",
        "outputId": "8be11832-50fc-400b-94b9-1fa8fec620ab"
      },
      "outputs": [],
      "source": [
        "nfl_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_W6oH2695aB"
      },
      "source": [
        "¿Observamos datos faltantes?\n",
        "\n",
        "¿Cuántos puntos de datos faltantes tenemos?\n",
        "\n",
        "Bien, ahora sabemos que tenemos algunos valores faltantes. Veamos cuántos tenemos en cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DrU9z_795aB",
        "outputId": "45b3c008-7475-4724-9ed9-e63a32bc7606"
      },
      "outputs": [],
      "source": [
        "# obtenemos el número de datos faltantes por columna\n",
        "missing_values_count = nfl_data.isnull().sum()\n",
        "\n",
        "# Revisamos el número de datos faltantes en las primeras 10 columnas del conjunto de datos (tiene 102 columnas en total).\n",
        "missing_values_count[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pVqW_Qr95aC"
      },
      "source": [
        "¿Qué opinas de los resultados mostrados? Sería útil saber qué porcentaje de valores faltan en nuestro conjunto de datos para hacernos una idea más precisa de la magnitud del problema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gACtEoRaOguR",
        "outputId": "9b8e94cf-f429-4b54-e169-35ac6bad254e"
      },
      "outputs": [],
      "source": [
        "print(nfl_data.shape)\n",
        "a,b = nfl_data.shape\n",
        "print(a)\n",
        "print(b)\n",
        "print(a*b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5EXLnNs95aC",
        "outputId": "0b005414-9fa0-40f0-e559-9e2cffc6d87e"
      },
      "outputs": [],
      "source": [
        "# ¿Cuántos valores faltantes tenemos en total en el conjunto datos?\n",
        "print(nfl_data.shape)\n",
        "total_cells = np.product(nfl_data.shape)\n",
        "total_missing = missing_values_count.sum()\n",
        "\n",
        "# porcentaje de datos faltante\n",
        "percent_missing = (total_missing/total_cells) * 100\n",
        "print(f'Celdas totales: {total_cells:,}')   # Se agrega :, a la derecha de la variable para dar formato de miles\n",
        "print(f'Celdas con datos faltantes: {total_missing:,}') # Se agrega :, a la derecha de la variable para dar formato de miles\n",
        "print(f'Porcentaje de datos faltantes: {round(percent_missing,2)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWLqWpQ995aC"
      },
      "source": [
        "¿Qué opinas del porcentaje de datos faltantes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGtzGiAz95aD"
      },
      "source": [
        "#### Averiguar por qué faltan datos\n",
        "\n",
        "Este es el punto en el que entramos en la parte de la ciencia de datos que solemos llamar \"intuición de datos\", es decir, \"analizar realmente los datos e intentar averiguar por qué son como son y cómo afectarán a nuestro análisis\". Puede ser una parte frustrante de la ciencia de datos, especialmente si eres nuevo en este campo y no tienes mucha experiencia. Para tratar los valores que faltan, tendrás que usar tu intuición para averiguar por qué falta el valor. Una de las preguntas más importantes que puede hacerse para averiguarlo es la siguiente:\n",
        "\n",
        "**¿Este valor falta porque no se registró o porque no existe?**\n",
        "\n",
        "Si falta un valor porque no existe (como la altura del hijo mayor de alguien que no tiene hijos), no tiene sentido intentar adivinar cuál podría ser. Estos valores probablemente quieras mantenerlos como NaN. Por otro lado, si falta un valor porque no se registró, puede intentar adivinar cuál podría haber sido basándose en los demás valores de esa columna y fila. Esto se llama imputación, ¡y aprenderemos a hacerlo a continuación! :)\n",
        "\n",
        "Veamos un ejemplo. Observando el número de valores faltantes en el marco de datos nfl_data, nos damos cuenta de que la columna \"TimeSecs\" tiene muchos valores faltantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPBHFJTf95aD",
        "outputId": "acb9e667-0977-432f-f05e-25413fa0384a"
      },
      "outputs": [],
      "source": [
        "# Revisamos el número de datos faltantes en las primeras 10 columnas del conjunto de datos (tiene 102 columnas en total).\n",
        "missing_values_count[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR_eAYIh95aD"
      },
      "source": [
        "Revisando la documentación, podemos ver que esta columna tiene información sobre el número de segundos que quedaban en el partido cuando se hizo la jugada. Esto significa que estos valores probablemente faltan porque no se registraron, y no porque no existan. Por lo tanto, tendría sentido que intentáramos adivinar cuáles deberían ser en lugar de dejarlos como `NaN`.\n",
        "\n",
        "Por otra parte, hay otros campos, como \"PenalizedTeam\", en los que también faltan muchos campos. En este caso, sin embargo, el campo falta porque si no hubo penalización no tiene sentido decir qué equipo fue penalizado. Para esta columna, tendría más sentido dejarla vacía o añadir un tercer valor como \"ninguno\" y utilizarlo para reemplazar los `NaN`.\n",
        "\n",
        "Si está realizando un análisis de datos muy cuidadoso, este es el punto en el que miraría cada columna individualmente para averiguar cuál es la mejor estrategia para rellenar los valores que faltan. En el resto de este cuaderno, trataremos algunas técnicas \"rápidas y sucias\" que pueden ayudarle con los valores que faltan, pero que probablemente acabarán eliminando información útil o añadiendo ruido a los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8bNN-dA95aD"
      },
      "source": [
        "#### Eliminar valores faltantes\n",
        "Si tiene prisa o no tiene motivos para averiguar por qué faltan valores, una opción es eliminar las filas o columnas que contengan valores que falten. (Nota: ¡generalmente no se recomienda este enfoque para proyectos importantes! Suele merecer la pena tomarse el tiempo necesario para revisar los datos y examinar una por una todas las columnas con valores faltantes para conocer realmente el conjunto de datos).\n",
        "\n",
        "Si está seguro de que quiere eliminar las filas con valores faltantes, pandas tiene una función muy útil, dropna() para ayudarle a hacerlo. Vamos a probarla en nuestro conjunto de datos de la NFL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "aCC-LbXn95aD",
        "outputId": "3530f173-a45e-4a24-8ac4-282239c38f05"
      },
      "outputs": [],
      "source": [
        "# eliminar todos los renglones que contengan un valor faltante u omitido\n",
        "nfl_data.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU52YwaO95aD"
      },
      "source": [
        "¿Qué sucedió? ¡parece que se han eliminado todos nuestros datos! 😱 Esto se debe a que cada fila de nuestro conjunto de datos tenía al menos un valor faltante. Podríamos tener mejor suerte eliminando todas las columnas que tienen al menos un valor faltante en su lugar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "MTSMg6Oh95aE",
        "outputId": "28fde50b-4da4-4707-d2c4-d705437f4281"
      },
      "outputs": [],
      "source": [
        "# Eliminemos ahora todas las columnas en donde exista un valor faltante\n",
        "columns_with_na_dropped = nfl_data.dropna(axis=1) #Al agregar el parámetro axis=1 estamos indicando que el criterio sea revisar columnas. Por defecto se revisan renglones (axis=0)\n",
        "columns_with_na_dropped.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F9Zx2fy95aE",
        "outputId": "78cde983-37d2-4c2d-f1b9-a82c2c3d376d"
      },
      "outputs": [],
      "source": [
        "columns_with_na_dropped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDbH8ctBTFuM",
        "outputId": "d859dd22-6a64-491f-c226-b44e930a566d"
      },
      "outputs": [],
      "source": [
        "#a, b = nfl_data.shape\n",
        "a = nfl_data.shape[0]\n",
        "b = nfl_data.shape[1]\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbmwdhos95aE"
      },
      "source": [
        "Calcular cuanta información hemos perdido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6gO59CL95aE",
        "outputId": "91d408be-1f67-4a10-88c9-d3b030d32ec8"
      },
      "outputs": [],
      "source": [
        "print(f\"Columnas en el conjunto de datos original: {nfl_data.shape[1]}\")\n",
        "print(f\"Columnas restantes tras eliminar datos faltantes: {columns_with_na_dropped.shape[1]}\")\n",
        "print(f\"Total de columnas eliminadas: {nfl_data.shape[1] - columns_with_na_dropped.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoL3EIta95aE"
      },
      "source": [
        "#### Rellenar automáticamente los valores que faltan\n",
        "Otra opción es intentar rellenar los valores que faltan. Para ello, vamos a tomar una pequeña subsección de los datos de la NFL para que se imprima bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "_BMKzKX_95aF",
        "outputId": "5639d070-ece2-4a87-cb99-4dae3a7cf68b"
      },
      "outputs": [],
      "source": [
        "# obtenemos un pequeño subconjunto del conjunto de datos de la NFL\n",
        "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\n",
        "subset_nfl_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Jw-Jfa95aF"
      },
      "source": [
        "Podemos utilizar la función fillna() de Panda para que rellene por nosotros los valores que faltan en un marco de datos. Una opción que tenemos es especificar con qué queremos que se sustituyan los valores NaN. Aquí, estoy diciendo que me gustaría reemplazar todos los valores NaN con 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "S2Hbb2AS95aF",
        "outputId": "017d3638-ba64-4579-f108-9800201709db"
      },
      "outputs": [],
      "source": [
        "# remplazar todos los datos NaN con 0\n",
        "subset_nfl_data.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_D13Epg95aF"
      },
      "source": [
        "Quizás una mejor estrategia sea sustituir los valores que faltan por cualquier valor que le siga directamente en la misma columna. (Esto tiene mucho sentido para conjuntos de datos en los que las observaciones tienen algún tipo de orden lógico)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Eb6YJfNg95aF",
        "outputId": "bed43cc5-1fac-44bc-9e4c-73698bb84008"
      },
      "outputs": [],
      "source": [
        "# reemplazar todos los NaN con el valor que viene directamente después de él en la misma columna\n",
        "# y sustituir todos los NaN restantes por 0\n",
        "subset_nfl_data.fillna(method='bfill', axis=0).fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXpxJVHB95aF"
      },
      "source": [
        "Opciones para el parámetro `method`:\n",
        "\n",
        "**ffill**\n",
        "\n",
        "Rellenar valores propagando la última observación válida a la siguiente válida.\n",
        "\n",
        "**bfill**\n",
        "\n",
        "Rellenar valores utilizando la siguiente observación válida para rellenar el hueco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "2YeFf_XYVjPr",
        "outputId": "93390419-f384-44bf-d11d-82ce3ac3cd5f"
      },
      "outputs": [],
      "source": [
        "# reemplazar todos los NaN con el valor que viene directamente antes de él en la misma columna\n",
        "# y sustituir todos los NaN restantes por 0\n",
        "subset_nfl_data.fillna(method='ffill', axis=0).fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7QsTC8M95aF"
      },
      "source": [
        "### Escalado y Normalización\n",
        "Transformar variables numéricas para que tengan propiedades útiles.\n",
        "\n",
        "#### Cargamos los módulos a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvoJILGl95aF",
        "outputId": "bf9a7e3b-ef35-4dbe-f51a-6a0624eb528b"
      },
      "outputs": [],
      "source": [
        "%pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-apKtOUV95aG"
      },
      "outputs": [],
      "source": [
        "# módulos a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# para transformación Box-Cox\n",
        "from scipy import stats\n",
        "\n",
        "# para escalado min_max\n",
        "from mlxtend.preprocessing import minmax_scaling\n",
        "\n",
        "# módulos de visualización\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fijamos la semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcTUe-d095aG"
      },
      "source": [
        "#### Escalado frente a normalización: ¿Cuál es la diferencia?\n",
        "\n",
        "Una de las razones por las que es fácil confundirse entre escalado y normalización es porque los términos a veces se utilizan indistintamente y, para hacerlo aún más confuso, ¡son muy similares! En ambos casos, se transforman los valores de las variables numéricas para que los puntos de datos transformados tengan propiedades útiles específicas. La diferencia es que en el escalado, se cambia el rango de los datos, mientras que en la normalización, cambia la forma de la distribución de los datos.\n",
        "\n",
        "Hablemos un poco más en profundidad de cada una de estas opciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_34rZS195aG"
      },
      "source": [
        "#### Escalado\n",
        "\n",
        "Esto significa que está transformando sus datos para que se ajusten a una escala específica, como 0-100 o 0-1. Es conveniente escalar los datos cuando se utilizan métodos basados en medidas de la distancia entre los puntos de datos, como las máquinas de vectores de soporte (SVM) o los vecinos más cercanos (KNN). Con estos algoritmos, un cambio de \"1\" en cualquier característica numérica recibe la misma importancia.\n",
        "\n",
        "Por ejemplo, puede consultar los precios de algunos productos en yenes y en dólares estadounidenses. Un dólar estadounidense vale unos 100 yenes, pero si no escala los precios, métodos como SVM o KNN considerarán que una diferencia de precio de 1 yen es tan importante como una diferencia de 1 dólar estadounidense. Está claro que esto no encaja con nuestras intuiciones del mundo. Con la divisa, puedes convertir entre divisas. Pero, ¿qué ocurre con la altura y el peso? No está del todo claro cuántas libras equivalen a una pulgada (o cuántos kilogramos equivalen a un metro).\n",
        "\n",
        "Al escalar las variables, puedes comparar diferentes variables en igualdad de condiciones. Para ayudarte a entender cómo es el escalado, veamos un ejemplo inventado. (No te preocupes, en el siguiente ejercicio trabajaremos con datos reales)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "SnuPh2SE95aG",
        "outputId": "c11b5f75-211c-4c24-ac8f-a38900a29ad0"
      },
      "outputs": [],
      "source": [
        "# generar 1000 puntos de datos extraídos aleatoriamente de una distribución exponencial\n",
        "original_data = np.random.exponential(size=1000)\n",
        "\n",
        "# mix-max escala los datos entre 0 y 1\n",
        "scaled_data = minmax_scaling(original_data, columns=[0])\n",
        "\n",
        "# graficamos ambos para comparar\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
        "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
        "ax[0].set_title(\"Datos Originales\")\n",
        "sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)\n",
        "ax[1].set_title(\"Datos Escalados\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIN8liZR95aG"
      },
      "source": [
        "Observe que la forma de los datos no cambia, pero que en lugar de ir de 0 a 8, ahora van de 0 a 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK7oeRww95aG"
      },
      "source": [
        "#### Normalización\n",
        "El escalado sólo cambia el rango de los datos. La normalización es una transformación más radical. El objetivo de la normalización es cambiar las observaciones para que puedan describirse como una distribución normal.\n",
        "\n",
        "[Distribución normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal): También conocida como \"curva de campana\", es una distribución estadística específica en la que aproximadamente el mismo número de observaciones se sitúan por encima y por debajo de la media, la media y la mediana son iguales y hay más observaciones cerca de la media. La distribución normal también se conoce como distribución de Gauss.\n",
        "\n",
        "En general, normalizará sus datos si va a utilizar una técnica de aprendizaje automático o estadística que asuma que sus datos se distribuyen normalmente. Algunos ejemplos son el análisis discriminante lineal (LDA) y el Bayes ingenuo gaussiano. (Consejo profesional: cualquier método con \"gaussiano\" en el nombre probablemente asume la normalidad).\n",
        "\n",
        "El método que estamos utilizando para normalizar aquí se llama Transformación [Box-Cox](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Echemos un vistazo rápido a cómo se ve la normalización de algunos datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "nPk0p5VN95aG",
        "outputId": "da48ea57-5b2d-4000-8d4c-d4dbd79f711c"
      },
      "outputs": [],
      "source": [
        "# normalizar los datos exponenciales con boxcox\n",
        "normalized_data = stats.boxcox(original_data)\n",
        "\n",
        "# graficamos ambos para comparar\n",
        "fig, ax=plt.subplots(1, 2, figsize=(15, 3))\n",
        "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
        "ax[0].set_title(\"Datos Originales\")\n",
        "sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False)\n",
        "ax[1].set_title(\"Datos Normalizados\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DXNyKjD95aH"
      },
      "source": [
        "Observe que la forma de nuestros datos ha cambiado. Antes de la normalización tenían casi forma de L. Pero después de la normalización se parecen más al contorno de una campana (de ahí lo de \"curva de campana\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de fechas\n",
        "Ayuda a Python a reconocer fechas compuestas por día, mes y año.\n",
        "\n",
        "#### Cargamos los módulos y conjunto de datos a utilizar\n",
        "Trabajaremos con un conjunto de datos que contiene información sobre los desprendimientos de tierra ocurridos entre 2007 y 2016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# módulos a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "# cargamos los datos\n",
        "landslides_filepath = \"https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/landslides.csv\"\n",
        "landslides = pd.read_csv(landslides_filepath)\n",
        "\n",
        "# fijamos la semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comprobar el tipo de datos de nuestra columna de fecha\n",
        "Empezaremos echando un vistazo a las cinco primeras filas de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "landslides.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estaremos trabajando con la columna \"date\". Revisemos que realmente contenga fechas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imprimir los primeros renglones de la columna \"date\"\n",
        "print(landslides['date'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al ver los datos podemos asumir como humanos que son datos de fechas, pero esto no significa que Python reconozca que son fechas. Si revisamos la última línea de la función head() notamos que el tipo de dato (dtype) es \"object\".\n",
        "\n",
        "Si revisamos la documentación de dtype de pandas, veremos que también hay un dtype específico datetime64. Como el dtype de nuestra columna es object y no datetime64, podemos decir que Python no sabe que esta columna contiene fechas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También podemos ver sólo el dtype de una columna sin imprimir las primeras filas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Podemos revisar el tipo de dato de una columna\n",
        "landslides['date'].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"O\" es el código de \"objeto\", por lo que podemos ver que estos dos métodos nos dan la misma información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# También podemos revisar los tipos de dato de todas las columnas\n",
        "landslides.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convertir nuestras columnas de fecha a datetime\n",
        "Ahora que sabemos que nuestra columna de fecha no está siendo reconocida como una fecha, es hora de convertirla para que sea reconocida como una fecha. Esto se llama \"analizar fechas\" (\"parsing dates\" en inglés) porque estamos tomando una cadena e identificando las partes que la componen.\n",
        "\n",
        "Podemos determinar cuál es el formato de nuestras fechas con una guía llamada \"directiva strftime\", de la que puedes encontrar más información [aquí](https://strftime.org/)]. La idea básica es que hay que señalar donde están las diferentes partes de la fecha y qué signos de puntuación hay entre ellas. Hay muchas partes posibles de una fecha, pero las más comunes son %d para el día, %m para el mes, %y para un año de dos dígitos y %Y para un año de cuatro dígitos.\n",
        "\n",
        "Algunos ejemplos:\n",
        "\n",
        "- 1/17/07 tiene el formato \"%m/%d/%y\".\n",
        "- 17-1-2007 tiene el formato \"%d-%m-%Y\".\n",
        "\n",
        "Si volvemos a mirar la cabecera de la columna \"date\" (fecha) en el conjunto de datos de desprendimientos, vemos que tiene el formato \"month/day/two-digit year\" (mes/día/año de dos dígitos), por lo que podemos utilizar la misma sintaxis que en el primer ejemplo para analizar las fechas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crear una nueva columna, date_parsed, con las fechas analizadas\n",
        "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# revisamos los primeros datos\n",
        "landslides['date_parsed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que nuestras fechas están correctamente analizadas, podemos interactuar con ellas de forma útil.\n",
        "\n",
        "¿Qué pasa si nos encontramos con un error con múltiples formatos de fecha? Mientras estamos especificando el formato de fecha, a veces se encontrará con un error cuando hay múltiples formatos de fecha en una sola columna. Si eso ocurre, puede hacer que pandas intente deducir cuál debería ser el formato de fecha correcto. Puede hacerlo así:\n",
        "\n",
        "`landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)`\n",
        "\n",
        "¿Por qué no utilizar siempre infer_datetime_format = True? Hay dos grandes razones para no hacer que pandas adivine siempre el formato de hora. La primera es que pandas no siempre será capaz de averiguar el formato de fecha correcto, especialmente si alguien se ha puesto creativo con la entrada de datos. La segunda es que es mucho más lento que especificar el formato exacto de las fechas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seleccionar el día del mes\n",
        "Ahora que tenemos una columna de fechas analizadas, podemos extraer información como el día del mes en que se produjo un desprendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtener eñ día del mes de la columna date_parsed\n",
        "day_of_month_landslides = landslides['date_parsed'].dt.day\n",
        "day_of_month_landslides.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¿Qué sucede si intentamos hacer lo mismo con la columna \"date\" original?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "day_lanslides = landslides['date'].dt.day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trazar el día del mes para comprobar el análisis sintáctico de la fecha\n",
        "Uno de los mayores peligros al analizar fechas es confundir los meses y los días. La función to_datetime() tiene mensajes de error muy útiles, pero no está de más volver a comprobar que los días del mes que hemos extraído tienen sentido.\n",
        "\n",
        "Para ello, vamos a trazar un histograma de los días del mes. Esperamos que tenga valores entre 1 y 31 (Con un asterisco en el 31 porque no todos los meses tienen 31 días) y, puesto que no hay razón para suponer que los desprendimientos son más frecuentes en unos días del mes que en otros, esperamos una distribución relativamente uniforme. Veamos si es así:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remover valores nulos (NaN)\n",
        "day_of_month_landslides = day_of_month_landslides.dropna()\n",
        "\n",
        "# graficamos el día del mes\n",
        "sns.displot(day_of_month_landslides, kde=False, bins=31) #KDE (kernel density estimate) representa los datos mediante una curva de densidad de probabilidad continua en una o varias dimensiones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Codificación de caracteres\n",
        "Evite errores de descodificación Unicode al cargar archivos CSV.\n",
        "\n",
        "#### Cargamos los módulos que utilizaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# módulos básicos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# módulo de codificación de caracteres\n",
        "import charset_normalizer\n",
        "\n",
        "# fijar semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ¿Qué son las codificaciones?\n",
        "Las codificaciones de caracteres son conjuntos específicos de reglas para pasar de cadenas binarias de bytes en bruto (que tienen este aspecto: 0110100001101001) a caracteres que forman un texto legible por humanos (como \"hola\"). Hay muchas codificaciones diferentes, y si intentas leer un texto con una codificación diferente a la original, acabas con un texto confuso llamado \"mojibake\" (se dice mo-gee-bah-kay). He aquí un ejemplo de mojibake:\n",
        "\n",
        "\n",
        "æ-‡å-åŒ-ã??\n",
        "\n",
        "\n",
        "También puedes encontrarte con caracteres \"desconocidos\". Estos son los que se imprimen cuando no hay correspondencia entre un byte en particular y un caracter en la codificación que estás usando para leer tu cadena de bytes y se ven así:\n",
        "\n",
        "\n",
        "����������\n",
        "\n",
        "\n",
        "Los desajustes en la codificación de caracteres son menos comunes hoy en día de lo que solían ser, pero definitivamente siguen siendo un problema. Hay muchas codificaciones de caracteres diferentes, pero la principal que necesitas conocer es UTF-8.\n",
        "\n",
        "\n",
        "- UTF-8 es la codificación de texto estándar. Todo el código Python está en UTF-8 e, idealmente, todos tus datos deberían estarlo también. Es cuando las cosas no están en UTF-8 cuando tienes problemas.\n",
        "\n",
        "\n",
        "Era bastante difícil tratar con codificaciones en Python 2, pero afortunadamente en Python 3 es mucho más simple. (Los cuadernos Kaggle sólo usan Python 3.) Hay dos tipos de datos principales que encontrarás cuando trabajes con texto en Python 3. Uno es `string`, que es lo que el texto es por defecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iniciamos con un string\n",
        "before = \"This is the euro symbol: €\"\n",
        "\n",
        "# verificamos el tipo de dato\n",
        "type(before)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El otro dato es el tipo `bytes`, que es una secuencia de enteros. Puedes convertir una cadena en bytes especificando en qué codificación está:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# codificarlo a una codificación diferente, sustituyendo los caracteres que provocan errores\n",
        "after = before.encode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "# check the type\n",
        "print(type(after))\n",
        "print(after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si revisas un objeto `bytes`, verás que tiene una b delante, y quizás algo de texto después. Esto se debe a que los bytes se imprimen como si fueran caracteres codificados en ASCII. (ASCII es una codificación de caracteres antigua que realmente no funciona para escribir otro idioma que no sea el inglés). Aquí puedes ver que nuestro símbolo del euro ha sido reemplazado por algún mojibake que parece \"\\xe2\\x82\\xac\" cuando se imprime como si fuera una cadena ASCII."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando volvemos a convertir nuestros bytes en una cadena con la codificación correcta, podemos ver que nuestro texto está todo correctamente, ¡lo cual es genial! :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convertirlo de nuevo a utf-8\n",
        "print(after.decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, cuando intentamos utilizar una codificación diferente para convertir nuestros bytes en una cadena, obtenemos un error. Esto se debe a que la codificación que estamos intentando utilizar no sabe qué hacer con los bytes que estamos intentando pasarle. Necesitas decirle a Python la codificación en la que se supone que debe estar la cadena de bytes.\n",
        "\n",
        "Puedes pensar en diferentes codificaciones como diferentes formas de grabar música. Puedes grabar la misma música en un CD, en una cinta de casete o en una de 8 pistas. Aunque la música suene más o menos igual, hay que utilizar el equipo adecuado para reproducir la música de cada formato de grabación. El descodificador correcto es como un reproductor de casetes o de CD. Si intentas reproducir un casete en un reproductor de CD, no funcionará."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# intenta decodificarlo con codificación ascii\n",
        "print(after.decode(\"ascii\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También podemos tener problemas si intentamos usar la codificación incorrecta para pasar de una cadena a bytes. Como dije antes, las cadenas son UTF-8 por defecto en Python 3, así que si intentamos tratarlas como si estuvieran en otra codificación crearemos problemas.\n",
        "\n",
        "Por ejemplo, si intentamos convertir una cadena a bytes para ASCII usando encode(), podemos pedir que los bytes sean los que serían si el texto estuviera en ASCII. Sin embargo, como nuestro texto no está en ASCII, habrá algunos caracteres que no podrá manejar. Podemos reemplazar automáticamente los caracteres que ASCII no puede manejar. Sin embargo, si hacemos eso, cualquier carácter que no esté en ASCII será reemplazado por el carácter desconocido. Entonces, cuando convirtamos los bytes de nuevo en una cadena, el carácter será reemplazado por el carácter desconocido. La parte peligrosa de esto es que no hay forma de saber qué carácter debería haber sido. Esto significa que podemos haber convertido nuestros datos en algo inusable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iniciamos con un string\n",
        "before = \"This is the euro symbol: €\"\n",
        "\n",
        "# lo codificamos a un codificación diferente, remplazando los caracteres que marquen error\n",
        "after = before.encode(\"ascii\", errors = \"replace\")\n",
        "\n",
        "# lo convertimos de nuevo a utf-8\n",
        "print(after.decode(\"ascii\"))\n",
        "\n",
        "# ¡Hemos perdido la cadena de bytes subyacente original! \n",
        "# Ha sido reemplazada por la cadena de bytes subyacente para el carácter desconocido :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo anterior es malo y queremos evitarlo. Es mucho mejor convertir todo nuestro texto a UTF-8 tan pronto como podamos y mantenerlo en esa codificación. El mejor momento para convertir texto no UTF-8 a UTF-8 es cuando lees archivos, de lo que hablaremos a continuación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lectura de archivos con problemas de codificación\n",
        "\n",
        "La mayoría de los archivos que encuentres probablemente estarán codificados con UTF-8. Esto es lo que Python espera por defecto. Esto es lo que Python espera por defecto, así que la mayoría de las veces no tendrás problemas. Sin embargo, a veces aparecerá un error como éste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# intentemos leer un archivo que no está en UTF-8\n",
        "file_path = \"https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/ks-projects-201612.csv\"\n",
        "kickstarter_2016 = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intentemos detectar la codificación correcta del archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ocupamos tener de forma local el archivo para revisarlo\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/ks-projects-201612.csv -O ks-projects-201612.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# revisar los primeros diez mil bytes para inferir la codificación de caracteres\n",
        "with open(\"ks-projects-201612.csv\", 'rb') as rawdata:\n",
        "    result = charset_normalizer.detect(rawdata.read(10000))\n",
        "\n",
        "# verifiquemos que codificación podría ser la correcta\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que charset_normalizer tiene un 100% de confianza en que la codificación correcta es \"Windows-1252\". Veamos si eso es correcto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# leer el archivo con la codificación detectada por charset_normalizer\n",
        "kickstarter_2016 = pd.read_csv(\"data/ks-projects-201612.csv\", encoding='Windows-1252')\n",
        "\n",
        "# revisemos los primeros renglones\n",
        "kickstarter_2016.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¿Qué pasa si la codificación que infiere charset_normalizer no es la correcta? Como charset_normalizer es básicamente un adivinador elegante, a veces inferirá la codificación incorrecta. Una cosa que puedes intentar es mirar más o menos del archivo y ver si obtienes un resultado diferente y luego probar eso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Guardar tus archivos con codificación UTF-8\n",
        "Por último, una vez que te hayas tomado la molestia de convertir tu archivo a UTF-8, probablemente querrás mantenerlo así. La forma más sencilla de hacerlo es guardar tus archivos con codificación UTF-8. La buena noticia es que, dado que UTF-8 es la codificación estándar en Python, cuando guardes un archivo se guardará como UTF-8 por defecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# guardemos nuestro archivo (se guardará como UTF-8 por defecto)\n",
        "kickstarter_2016.to_csv(\"ks-projects-201612-utf8.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introducción de datos incoherentes\n",
        "Corrija eficazmente los errores tipográficos en sus datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cargamos los módulos y conjunto de datos que utilizaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# instalamos el módulo en caso de que no esté precargado\n",
        "%pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "# módulos que ocupamos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# módulos útiles\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import process\n",
        "import charset_normalizer\n",
        "\n",
        "# cargamos los datos\n",
        "file_path = \"https://raw.githubusercontent.com/vbatiz/intro-python/main/notebooks/data/pakistan_intellectual_capital.csv\"\n",
        "professors = pd.read_csv(file_path)\n",
        "professors = pd.read_csv(\"data/pakistan_intellectual_capital.csv\")\n",
        "\n",
        "# fijamos semilla para reproducibilidad\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Realicemos preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>S#</th>\n",
              "      <th>Teacher Name</th>\n",
              "      <th>University Currently Teaching</th>\n",
              "      <th>Department</th>\n",
              "      <th>Province University Located</th>\n",
              "      <th>Designation</th>\n",
              "      <th>Terminal Degree</th>\n",
              "      <th>Graduated from</th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Area of Specialization/Research Interests</th>\n",
              "      <th>Other Information</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>Dr. Abdul Basit</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Software Engineering &amp; DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Dr. Waheed Noor</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>Dr. Junaid Baber</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Information processing, Multimedia mining</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>Dr. Maheen Bakhtyar</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NLP, Information Retrieval, Question Answering...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>Samina Azim</td>\n",
              "      <td>Sardar Bahadur Khan Women's University</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Lecturer</td>\n",
              "      <td>BS</td>\n",
              "      <td>Balochistan University of Information Technolo...</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>VLSI Electronics DLD Database</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  S#         Teacher Name  \\\n",
              "0           2   3      Dr. Abdul Basit   \n",
              "1           4   5      Dr. Waheed Noor   \n",
              "2           5   6     Dr. Junaid Baber   \n",
              "3           6   7  Dr. Maheen Bakhtyar   \n",
              "4          24  25          Samina Azim   \n",
              "\n",
              "            University Currently Teaching             Department  \\\n",
              "0               University of Balochistan  Computer Science & IT   \n",
              "1               University of Balochistan  Computer Science & IT   \n",
              "2               University of Balochistan  Computer Science & IT   \n",
              "3               University of Balochistan  Computer Science & IT   \n",
              "4  Sardar Bahadur Khan Women's University       Computer Science   \n",
              "\n",
              "  Province University Located          Designation Terminal Degree  \\\n",
              "0                 Balochistan  Assistant Professor             PhD   \n",
              "1                 Balochistan  Assistant Professor             PhD   \n",
              "2                 Balochistan  Assistant Professor             PhD   \n",
              "3                 Balochistan  Assistant Professor             PhD   \n",
              "4                 Balochistan             Lecturer              BS   \n",
              "\n",
              "                                      Graduated from   Country    Year  \\\n",
              "0                      Asian Institute of Technology  Thailand     NaN   \n",
              "1                      Asian Institute of Technology  Thailand     NaN   \n",
              "2                      Asian Institute of Technology  Thailand     NaN   \n",
              "3                      Asian Institute of Technology  Thailand     NaN   \n",
              "4  Balochistan University of Information Technolo...  Pakistan  2005.0   \n",
              "\n",
              "           Area of Specialization/Research Interests Other Information  \n",
              "0                        Software Engineering & DBMS               NaN  \n",
              "1                                               DBMS               NaN  \n",
              "2          Information processing, Multimedia mining               NaN  \n",
              "3  NLP, Information Retrieval, Question Answering...               NaN  \n",
              "4                      VLSI Electronics DLD Database               NaN  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "professors.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Digamos que nos interesa limpiar la columna \"Country\" para asegurarnos de que no hay incoherencias en la introducción de datos. Podríamos revisar cada fila a mano, por supuesto, y corregir a mano las incoherencias que encontremos. Pero hay una forma más eficaz de hacerlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([' Germany', ' New Zealand', ' Sweden', ' USA', 'Australia',\n",
              "       'Austria', 'Canada', 'China', 'Finland', 'France', 'Greece',\n",
              "       'HongKong', 'Ireland', 'Italy', 'Japan', 'Macau', 'Malaysia',\n",
              "       'Mauritius', 'Netherland', 'New Zealand', 'Norway', 'Pakistan',\n",
              "       'Portugal', 'Russian Federation', 'Saudi Arabia', 'Scotland',\n",
              "       'Singapore', 'South Korea', 'SouthKorea', 'Spain', 'Sweden',\n",
              "       'Thailand', 'Turkey', 'UK', 'USA', 'USofA', 'Urbana', 'germany'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtengamos todos los valores distintos de la columna 'Country'\n",
        "countries = professors['Country'].unique()\n",
        "\n",
        "# ordenemos los nombres y revisemos detenidamente los datos\n",
        "countries.sort()\n",
        "countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¿Qué incosnsistencias observamos?\n",
        "\n",
        "Lo primero que haremos será pasar los textos a minúsculas y eliminar los espacios adicionales al inicio y final de los textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convertir a minúsculas\n",
        "professors['Country'] = professors['Country'].str.lower()\n",
        "\n",
        "# remover espacios en blanco al inicio y final\n",
        "professors['Country'] = professors['Country'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>S#</th>\n",
              "      <th>Teacher Name</th>\n",
              "      <th>University Currently Teaching</th>\n",
              "      <th>Department</th>\n",
              "      <th>Province University Located</th>\n",
              "      <th>Designation</th>\n",
              "      <th>Terminal Degree</th>\n",
              "      <th>Graduated from</th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Area of Specialization/Research Interests</th>\n",
              "      <th>Other Information</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>Dr. Abdul Basit</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Software Engineering &amp; DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Dr. Waheed Noor</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DBMS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>Dr. Junaid Baber</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Information processing, Multimedia mining</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>Dr. Maheen Bakhtyar</td>\n",
              "      <td>University of Balochistan</td>\n",
              "      <td>Computer Science &amp; IT</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Assistant Professor</td>\n",
              "      <td>PhD</td>\n",
              "      <td>Asian Institute of Technology</td>\n",
              "      <td>Thailand</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NLP, Information Retrieval, Question Answering...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>Samina Azim</td>\n",
              "      <td>Sardar Bahadur Khan Women's University</td>\n",
              "      <td>Computer Science</td>\n",
              "      <td>Balochistan</td>\n",
              "      <td>Lecturer</td>\n",
              "      <td>BS</td>\n",
              "      <td>Balochistan University of Information Technolo...</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>VLSI Electronics DLD Database</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  S#         Teacher Name  \\\n",
              "0           2   3      Dr. Abdul Basit   \n",
              "1           4   5      Dr. Waheed Noor   \n",
              "2           5   6     Dr. Junaid Baber   \n",
              "3           6   7  Dr. Maheen Bakhtyar   \n",
              "4          24  25          Samina Azim   \n",
              "\n",
              "            University Currently Teaching             Department  \\\n",
              "0               University of Balochistan  Computer Science & IT   \n",
              "1               University of Balochistan  Computer Science & IT   \n",
              "2               University of Balochistan  Computer Science & IT   \n",
              "3               University of Balochistan  Computer Science & IT   \n",
              "4  Sardar Bahadur Khan Women's University       Computer Science   \n",
              "\n",
              "  Province University Located          Designation Terminal Degree  \\\n",
              "0                 Balochistan  Assistant Professor             PhD   \n",
              "1                 Balochistan  Assistant Professor             PhD   \n",
              "2                 Balochistan  Assistant Professor             PhD   \n",
              "3                 Balochistan  Assistant Professor             PhD   \n",
              "4                 Balochistan             Lecturer              BS   \n",
              "\n",
              "                                      Graduated from   Country    Year  \\\n",
              "0                      Asian Institute of Technology  Thailand     NaN   \n",
              "1                      Asian Institute of Technology  Thailand     NaN   \n",
              "2                      Asian Institute of Technology  Thailand     NaN   \n",
              "3                      Asian Institute of Technology  Thailand     NaN   \n",
              "4  Balochistan University of Information Technolo...  Pakistan  2005.0   \n",
              "\n",
              "           Area of Specialization/Research Interests Other Information  \n",
              "0                        Software Engineering & DBMS               NaN  \n",
              "1                                               DBMS               NaN  \n",
              "2          Information processing, Multimedia mining               NaN  \n",
              "3  NLP, Information Retrieval, Question Answering...               NaN  \n",
              "4                      VLSI Electronics DLD Database               NaN  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Otra forma de convertir a minúsculas es utilizando la función apply y str.lower.\n",
        "\n",
        "professors['Country'] = professors.Country.apply(str.capitalize)\n",
        "professors.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizar la concordancia difusa para corregir la introducción de datos incoherentes\n",
        "Muy bien, echemos otro vistazo a la columna \"Country\" y veamos si hay que limpiar más datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Australia', 'Austria', 'Canada', 'China', 'Finland', 'France',\n",
              "       'Germany', 'Greece', 'Hongkong', 'Ireland', 'Italy', 'Japan',\n",
              "       'Macau', 'Malaysia', 'Mauritius', 'Netherland', 'New zealand',\n",
              "       'Norway', 'Pakistan', 'Portugal', 'Russian federation',\n",
              "       'Saudi arabia', 'Scotland', 'Singapore', 'South korea',\n",
              "       'Southkorea', 'Spain', 'Sweden', 'Thailand', 'Turkey', 'Uk',\n",
              "       'Urbana', 'Usa', 'Usofa'], dtype=object)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtengamos todos los valores distintos de la columna 'Country'\n",
        "countries = professors['Country'].unique()\n",
        "\n",
        "# ordenemos los nombres y revisemos detenidamente los datos\n",
        "countries.sort()\n",
        "countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parece que hay otra incoherencia: \"southkorea\" y \"south korea\" deberían ser lo mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a utilizar el paquete fuzzywuzzy para que nos ayude a identificar las cadenas que más se parecen entre sí. Este conjunto de datos es lo suficientemente pequeño como para que pudiéramos corregir los errores a mano, pero este método no es muy adecuado. (¿Le gustaría corregir mil errores a mano? ¿Y diez mil? Automatizar las cosas lo antes posible suele ser una buena idea. Además, es divertido).\n",
        "\n",
        "**Coincidencia difusa**: proceso que consiste en encontrar automáticamente cadenas de texto muy similares a la cadena objetivo. En general, una cadena se considera \"más parecida\" a otra cuantos menos caracteres tendría que cambiar si transformara una cadena en otra. Así, \"apple\" y \"snapple\" están a dos cambios de distancia (añadir \"s\" y \"n\"), mientras que \"in\" y \"on\" están a un cambio (sustituir \"i\" por \"o\"). No siempre podrá confiar al 100% en la concordancia difusa, pero normalmente acabará ahorrándole al menos un poco de tiempo.\n",
        "\n",
        "Fuzzywuzzy devuelve un cociente dadas dos cadenas. Cuanto más se acerque el cociente a 100, menor será la distancia de edición entre las dos cadenas. Aquí, vamos a obtener las diez cadenas de nuestra lista de países que tienen la distancia más cercana a \"south korea\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('South korea', 100),\n",
              " ('Southkorea', 48),\n",
              " ('Saudi arabia', 43),\n",
              " ('Norway', 35),\n",
              " ('Ireland', 33),\n",
              " ('Portugal', 32),\n",
              " ('Singapore', 30),\n",
              " ('Netherland', 29),\n",
              " ('Macau', 25),\n",
              " ('Usofa', 25)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtener las 10 coincidencias más cercanas a \"south korea\"\n",
        "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
        "\n",
        "# revisemos\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que dos de los elementos de las ciudades están muy cerca de \"south korea\": \"south korea\" y \"southkorea\". Sustituyamos todas las filas de nuestra columna \"Country\" que tengan una proporción > 47 por \"south korea\".\n",
        "\n",
        "Para ello, vamos a escribir una función. (Es una buena idea escribir una función de propósito general que puedas reutilizar si crees que vas a tener que hacer una tarea específica más de una o dos veces. Esto evita que tengas que copiar y pegar código con demasiada frecuencia, lo que ahorra tiempo y puede ayudar a evitar errores)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# función para remplazar los textos de la columna indicada con base al nivel de coincidencia indicado\n",
        "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
        "    # obtenemos los valores únicos\n",
        "    strings = df[column].unique()\n",
        "    \n",
        "    # obtenemos los 10 valores más cercano a nuestra cadena\n",
        "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
        "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
        "\n",
        "    # filtramos solo las coincidencias con un valore de similitud > min_ratio\n",
        "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
        "\n",
        "    # obtenemos los registros (renglones) de las coincidencias más cercanas en el conjunto de datos\n",
        "    rows_with_matches = df[column].isin(close_matches)\n",
        "\n",
        "    # remplazamos los textos con el texto correcto\n",
        "    df.loc[rows_with_matches, column] = string_to_match\n",
        "    \n",
        "    # mostramos mensaje de que hemos finalizado\n",
        "    print(\"¡Listo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Listo!\n"
          ]
        }
      ],
      "source": [
        "# usamos la función para remplazar coincidencias cercanas a \"south korea\" con \"south korea\"\n",
        "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Revisamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Australia', 'Austria', 'Canada', 'China', 'Finland', 'France',\n",
              "       'Germany', 'Greece', 'Hongkong', 'Ireland', 'Italy', 'Japan',\n",
              "       'Macau', 'Malaysia', 'Mauritius', 'Netherland', 'New zealand',\n",
              "       'Norway', 'Pakistan', 'Portugal', 'Russian federation',\n",
              "       'Saudi arabia', 'Scotland', 'Singapore', 'Spain', 'Sweden',\n",
              "       'Thailand', 'Turkey', 'Uk', 'Urbana', 'Usa', 'Usofa',\n",
              "       'south korea'], dtype=object)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# obtengamos todos los valores distintos de la columna 'Country'\n",
        "countries = professors['Country'].unique()\n",
        "\n",
        "# ordenemos los nombres y revisemos detenidamente los datos\n",
        "countries.sort()\n",
        "countries"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
