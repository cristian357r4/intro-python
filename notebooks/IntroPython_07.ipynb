{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complejidad de los datos\n",
    "\n",
    "### Introducci√≥n\n",
    "\n",
    "En esta sesi√≥n analizaremos el tema de complejidad de los datos y algunas t√©cnicas para tratar los efectos de la misma. Este cuaderno se basa parcialmente en el material del curso de limpieza de datos de Kaggle disponible [aqu√≠](https://www.kaggle.com/learn/data-cleaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gesti√≥n de valores omitidos\n",
    "Elimine los valores que faltan o rell√©nelos con un flujo de trabajo automatizado.\n",
    "\n",
    "La limpieza de datos es una parte clave de la ciencia de datos, pero puede ser muy frustrante. ¬øPor qu√© hay campos de texto ilegibles? ¬øQu√© hacer con los valores que faltan? ¬øPor qu√© las fechas no tienen el formato correcto? ¬øC√≥mo puede solucionar r√°pidamente la introducci√≥n de datos incoherentes? En este tema, aprender√° por qu√© se ha encontrado con estos problemas y, lo que es m√°s importante, c√≥mo solucionarlos.\n",
    "\n",
    "En este cuaderno, aprender√° a abordar algunos de los problemas m√°s comunes de limpieza de datos para que pueda analizar sus datos m√°s r√°pidamente. Realizar√° cinco ejercicios pr√°cticos con datos reales y desordenados y responder√° a algunas de las preguntas m√°s frecuentes sobre la limpieza de datos.\n",
    "\n",
    "En este cuaderno, veremos c√≥mo tratar los valores faltantes u omitidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer vistazo a los datos\n",
    "\n",
    "Lo primero que tenemos que hacer es cargar las bibliotecas y el conjunto de datos que vamos a utilizar.\n",
    "\n",
    "Para la demostraci√≥n, utilizaremos un conjunto de datos de eventos ocurridos en partidos de f√∫tbol americano. Debido al tama√±o del conjunto de datos, lo descargaremos y posteriormente lo cargaremos a nuestro espacio temporal. [Ir a la p√°gina de descarga](https://www.kaggle.com/code/alexisbcook/handling-missing-values/data?select=NFL+Play+by+Play+2009-2017+%28v4%29.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m√≥dulos que usaremos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cargamos los datos\n",
    "nfl_data = pd.read_csv(\"NFL Play by Play 2009-2017 (v4).csv\")\n",
    "\n",
    "# fijamos la semilla para reproducibilidad\n",
    "np.random.seed(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hay que hacer cuando se recibe un nuevo conjunto de datos es echar un vistazo a algunos de ellos. Esto nos permite ver que todo se lee correctamente y nos da una idea de lo que est√° pasando con los datos. En este caso, vamos a ver si hay valores perdidos u omitidos, que son representados en Python con `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øObservamos datos faltantes?\n",
    "\n",
    "¬øCu√°ntos puntos de datos faltantes tenemos?\n",
    "\n",
    "Bien, ahora sabemos que tenemos algunos valores faltantes. Veamos cu√°ntos tenemos en cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos el n√∫mero de datos faltantes por columna\n",
    "missing_values_count = nfl_data.isnull().sum()\n",
    "\n",
    "# Revisamos el n√∫mero de datos faltantes en las primeras 10 columnas del conjunto de datos (tiene 102 columnas en total).\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øQu√© opinas de los resultados mostrados? Ser√≠a √∫til saber qu√© porcentaje de valores faltan en nuestro conjunto de datos para hacernos una idea m√°s precisa de la magnitud del problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬øCu√°ntos valores faltantes tenemos en total en el conjunto datos?\n",
    "print(nfl_data.shape)\n",
    "total_cells = np.product(nfl_data.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# porcentaje de datos faltante\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "print(f'Celdas totales: {total_cells:,}')   # Se agrega :, a la derecha de la variable para dar formato de miles\n",
    "print(f'Celdas con datos faltantes: {total_missing:,}') # Se agrega :, a la derecha de la variable para dar formato de miles\n",
    "print(f'Porcentaje de datos faltantes: {round(percent_missing,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øQu√© opinas del porcentaje de datos faltantes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averiguar por qu√© faltan datos\n",
    "\n",
    "Este es el punto en el que entramos en la parte de la ciencia de datos que solemos llamar \"intuici√≥n de datos\", es decir, \"analizar realmente los datos e intentar averiguar por qu√© son como son y c√≥mo afectar√°n a nuestro an√°lisis\". Puede ser una parte frustrante de la ciencia de datos, especialmente si eres nuevo en este campo y no tienes mucha experiencia. Para tratar los valores que faltan, tendr√°s que usar tu intuici√≥n para averiguar por qu√© falta el valor. Una de las preguntas m√°s importantes que puede hacerse para averiguarlo es la siguiente:\n",
    "\n",
    "**¬øEste valor falta porque no se registr√≥ o porque no existe?**\n",
    "\n",
    "Si falta un valor porque no existe (como la altura del hijo mayor de alguien que no tiene hijos), no tiene sentido intentar adivinar cu√°l podr√≠a ser. Estos valores probablemente quieras mantenerlos como NaN. Por otro lado, si falta un valor porque no se registr√≥, puede intentar adivinar cu√°l podr√≠a haber sido bas√°ndose en los dem√°s valores de esa columna y fila. Esto se llama imputaci√≥n, ¬°y aprenderemos a hacerlo a continuaci√≥n! :)\n",
    "\n",
    "Veamos un ejemplo. Observando el n√∫mero de valores faltantes en el marco de datos nfl_data, nos damos cuenta de que la columna \"TimesSec\" tiene muchos valores faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos el n√∫mero de datos faltantes en las primeras 10 columnas del conjunto de datos (tiene 102 columnas en total).\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisando la documentaci√≥n, podemos ver que esta columna tiene informaci√≥n sobre el n√∫mero de segundos que quedaban en el partido cuando se hizo la jugada. Esto significa que estos valores probablemente faltan porque no se registraron, y no porque no existan. Por lo tanto, tendr√≠a sentido que intent√°ramos adivinar cu√°les deber√≠an ser en lugar de dejarlos como `NaN`.\n",
    "\n",
    "Por otra parte, hay otros campos, como \"PenalizedTeam\", en los que tambi√©n faltan muchos campos. En este caso, sin embargo, el campo falta porque si no hubo penalizaci√≥n no tiene sentido decir qu√© equipo fue penalizado. Para esta columna, tendr√≠a m√°s sentido dejarla vac√≠a o a√±adir un tercer valor como \"ninguno\" y utilizarlo para reemplazar los `NaN`.\n",
    "\n",
    "Si est√° realizando un an√°lisis de datos muy cuidadoso, este es el punto en el que mirar√≠a cada columna individualmente para averiguar cu√°l es la mejor estrategia para rellenar los valores que faltan. En el resto de este cuaderno, trataremos algunas t√©cnicas \"r√°pidas y sucias\" que pueden ayudarle con los valores que faltan, pero que probablemente acabar√°n eliminando informaci√≥n √∫til o a√±adiendo ruido a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar valores faltantes\n",
    "Si tiene prisa o no tiene motivos para averiguar por qu√© faltan valores, una opci√≥n es eliminar las filas o columnas que contengan valores que falten. (Nota: ¬°generalmente no se recomienda este enfoque para proyectos importantes! Suele merecer la pena tomarse el tiempo necesario para revisar los datos y examinar una por una todas las columnas con valores faltantes para conocer realmente el conjunto de datos).\n",
    "\n",
    "Si est√° seguro de que quiere eliminar las filas con valores faltantes, pandas tiene una funci√≥n muy √∫til, dropna() para ayudarle a hacerlo. Vamos a probarla en nuestro conjunto de datos de la NFL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar todos los renglones que contengan un valor faltante u omitido\n",
    "nfl_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øQu√© sucedi√≥? ¬°parece que se han eliminado todos nuestros datos! üò± Esto se debe a que cada fila de nuestro conjunto de datos ten√≠a al menos un valor faltante. Podr√≠amos tener mejor suerte eliminando todas las columnas que tienen al menos un valor faltante en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminemos ahora todas las columnas en donde exista un valor faltante\n",
    "columns_with_na_dropped = nfl_data.dropna(axis=1) #Al agregar el par√°metro axis=1 estamos indicando que el criterio sea revisar columnas. Por defecto se revisan renglones (axis=0)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_na_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular cuanta informaci√≥n hemos perdido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Columnas en el conjunto de datos original: {nfl_data.shape[1]}\")\n",
    "print(f\"Columnas restantes tras eliminar datos faltantes: {columns_with_na_dropped.shape[1]}\")\n",
    "print(f\"Total de columnas eliminadas: {nfl_data.shape[1] - columns_with_na_dropped.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rellenar autom√°ticamente los valores que faltan\n",
    "Otra opci√≥n es intentar rellenar los valores que faltan. Para ello, vamos a tomar una peque√±a subsecci√≥n de los datos de la NFL para que se imprima bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos un peque√±o subconjunto del conjunto de datos de la NFL\n",
    "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\n",
    "subset_nfl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar la funci√≥n fillna() de Panda para que rellene por nosotros los valores que faltan en un marco de datos. Una opci√≥n que tenemos es especificar con qu√© queremos que se sustituyan los valores NaN. Aqu√≠, estoy diciendo que me gustar√≠a reemplazar todos los valores NaN con 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplazar todos los datos NaN con 0\n",
    "subset_nfl_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz√°s una mejor estrategia sea sustituir los valores que faltan por cualquier valor que le siga directamente en la misma columna. (Esto tiene mucho sentido para conjuntos de datos en los que las observaciones tienen alg√∫n tipo de orden l√≥gico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazar todos los NaN el valor que viene directamente despu√©s de √©l en la misma columna\n",
    "# y sustituir todos los NaN restantes por 0\n",
    "subset_nfl_data.fillna(method='bfill', axis=0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras opciones para el par√°metro `method`:\n",
    "\n",
    "**ffill**\n",
    "\n",
    "Rellenar valores propagando la √∫ltima observaci√≥n v√°lida a la siguiente v√°lida.\n",
    "\n",
    "**bfill**\n",
    "\n",
    "Rellenar valores utilizando la siguiente observaci√≥n v√°lida para rellenar el hueco.\n",
    "\n",
    "**interpolate**\n",
    "\n",
    "Rellena valores NaN usando interpolaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado y Normalizaci√≥n\n",
    "Transformar variables num√©ricas para que tengan propiedades √∫tiles.\n",
    "\n",
    "#### Cargamos los m√≥dulos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m√≥dulos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# para transformaci√≥n Box-Cox\n",
    "from scipy import stats\n",
    "\n",
    "# para escalado min_max\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# m√≥dulos de visualizaci√≥n\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fijamos la semilla para reproducibilidad\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado frente a normalizaci√≥n: ¬øCu√°l es la diferencia?\n",
    "\n",
    "Una de las razones por las que es f√°cil confundirse entre escalado y normalizaci√≥n es porque los t√©rminos a veces se utilizan indistintamente y, para hacerlo a√∫n m√°s confuso, ¬°son muy similares! En ambos casos, se transforman los valores de las variables num√©ricas para que los puntos de datos transformados tengan propiedades √∫tiles espec√≠ficas. La diferencia es que en el escalado, se cambia el rango de los datos, mientras que en la normalizaci√≥n, cambia la forma de la distribuci√≥n de los datos.\n",
    "\n",
    "Hablemos un poco m√°s en profundidad de cada una de estas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado\n",
    "\n",
    "Esto significa que est√° transformando sus datos para que se ajusten a una escala espec√≠fica, como 0-100 o 0-1. Es conveniente escalar los datos cuando se utilizan m√©todos basados en medidas de la distancia entre los puntos de datos, como las m√°quinas de vectores de soporte (SVM) o los vecinos m√°s cercanos (KNN). Con estos algoritmos, un cambio de \"1\" en cualquier caracter√≠stica num√©rica recibe la misma importancia.\n",
    "\n",
    "Por ejemplo, puede consultar los precios de algunos productos en yenes y en d√≥lares estadounidenses. Un d√≥lar estadounidense vale unos 100 yenes, pero si no escala los precios, m√©todos como SVM o KNN considerar√°n que una diferencia de precio de 1 yen es tan importante como una diferencia de 1 d√≥lar estadounidense. Est√° claro que esto no encaja con nuestras intuiciones del mundo. Con la divisa, puedes convertir entre divisas. Pero, ¬øqu√© ocurre con la altura y el peso? No est√° del todo claro cu√°ntas libras equivalen a una pulgada (o cu√°ntos kilogramos equivalen a un metro).\n",
    "\n",
    "Al escalar las variables, puedes comparar diferentes variables en igualdad de condiciones. Para ayudarte a entender c√≥mo es el escalado, veamos un ejemplo inventado. (No te preocupes, en el siguiente ejercicio trabajaremos con datos reales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar 1000 puntos de datos extra√≠dos aleatoriamente de una distribuci√≥n exponencial\n",
    "original_data = np.random.exponential(size=1000)\n",
    "\n",
    "# mix-max escala los datos entre 0 y 1\n",
    "scaled_data = minmax_scaling(original_data, columns=[0])\n",
    "\n",
    "# graficamos ambos para comparar\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Datos Originales\")\n",
    "sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Datos Escalados\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que la forma de los datos no cambia, pero que en lugar de ir de 0 a 8, ahora van de 0 a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizaci√≥n\n",
    "El escalado s√≥lo cambia el rango de los datos. La normalizaci√≥n es una transformaci√≥n m√°s radical. El objetivo de la normalizaci√≥n es cambiar las observaciones para que puedan describirse como una distribuci√≥n normal.\n",
    "\n",
    "[Distribuci√≥n normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal): Tambi√©n conocida como \"curva de campana\", es una distribuci√≥n estad√≠stica espec√≠fica en la que aproximadamente el mismo n√∫mero de observaciones se sit√∫an por encima y por debajo de la media, la media y la mediana son iguales y hay m√°s observaciones cerca de la media. La distribuci√≥n normal tambi√©n se conoce como distribuci√≥n de Gauss.\n",
    "\n",
    "En general, normalizar√° sus datos si va a utilizar una t√©cnica de aprendizaje autom√°tico o estad√≠stica que asuma que sus datos se distribuyen normalmente. Algunos ejemplos son el an√°lisis discriminante lineal (LDA) y el Bayes ingenuo gaussiano. (Consejo profesional: cualquier m√©todo con \"gaussiano\" en el nombre probablemente asume la normalidad).\n",
    "\n",
    "El m√©todo que estamos utilizando para normalizar aqu√≠ se llama Transformaci√≥n [Box-Cox](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Echemos un vistazo r√°pido a c√≥mo se ve la normalizaci√≥n de algunos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar los datos exponenciales con boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# graficamos ambos para comparar\n",
    "fig, ax=plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(original_data, ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Datos Originales\")\n",
    "sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Datos Normalizados\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que la forma de nuestros datos ha cambiado. Antes de la normalizaci√≥n ten√≠an casi forma de L. Pero despu√©s de la normalizaci√≥n se parecen m√°s al contorno de una campana (de ah√≠ lo de \"curva de campana\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
